{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from collections import Counter\n",
    "import soundfile as sf                                                     \n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import random\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose 25 actors from 158 actors\n",
    "df = pd.DataFrame(x)\n",
    "df['actor'] = df['root'].apply(lambda x: x.split('_')[2])\n",
    "df['condition'] = df['root'].apply(lambda x: 'zoom' if 'z' == x.split('/')[3][0] else 'studio')\n",
    "df['microphone'] = df['root'].apply(lambda x: x.split('_')[1])\n",
    "samples = random.sample(list(df['actor'].unique()),k=25)\n",
    "test = df[df['actor'].isin(samples)].reset_index(drop=True)\n",
    "print(test['emotion'].value_counts())\n",
    "train = df[~df['actor'].isin(samples)].reset_index(drop=True)\n",
    "test.to_csv('test_actor.csv',index=False)\n",
    "train.to_csv('train_actor.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority_eval(pred_sequences):\n",
    "    out = []\n",
    "    for i in pred_sequences:\n",
    "        emotions = dict(Counter(i))\n",
    "        y_pred = sorted(emotions, key=lambda k: emotions[k],reverse=True)[0]\n",
    "        out.append(y_pred)\n",
    "    return out\n",
    "\n",
    "def read_feature(fname, offset, sr=8000, frame_size=0.6, hop=0.3, n_fft=200, hop_length=80):\n",
    "    tmp_frame_size = int(frame_size*sr)\n",
    "    wav_data, _ = librosa.load(fname, sr=sr, res_type='kaiser_fast',duration=frame_size, offset=offset)\n",
    "    wav_data = librosa.effects.preemphasis(wav_data)\n",
    "    if wav_data.shape[0] != tmp_frame_size:\n",
    "        wav_data = np.pad(wav_data, (0, tmp_frame_size - wav_data.shape[0]), 'constant', constant_values=(0, 0))\n",
    "    if tmp_frame_size != wav_data.shape[0]:\n",
    "        print('bug dimension!!!!!', fname)\n",
    "    return librosa.feature.melspectrogram(y=wav_data, sr=sr, n_fft=n_fft, hop_length=hop_length)\n",
    "\n",
    "def prepare_test_set(filename, min_duration=0.3, sr=8000, frame_size=0.6,hop=0.3, n_fft=200, hop_length=80):\n",
    "    test_mapper = {'root': [], 'mel_spec': [],'start':[], 'end':[], 'emotion':[]}\n",
    "    test = pd.read_csv(filename)\n",
    "    with tqdm(total = len(test)) as pbar:\n",
    "        for row in test.itertuples(index=False):\n",
    "            if row.duration >= min_duration:\n",
    "                num_iter = int(1 + (row.duration - min_duration)//hop)\n",
    "                # print(row.duration, min_duration, hop, num_iter)\n",
    "                for i in range(num_iter):\n",
    "                    tmp_mel = read_feature(row.root, hop*i, sr=sr, frame_size=frame_size, hop=hop, n_fft=n_fft, hop_length=hop_length)\n",
    "                    test_mapper['root'].append(row.root)\n",
    "                    test_mapper['mel_spec'].append(tmp_mel)\n",
    "                    test_mapper['start'].append(hop*i)\n",
    "                    test_mapper['end'].append(min(hop*i+frame_size,row.duration))\n",
    "                    test_mapper['emotion'].append(row.emotion)\n",
    "            else:\n",
    "                print('very short file', row.root, root.duration)\n",
    "            pbar.update(1)\n",
    "    return test_mapper\n",
    "\n",
    "def prepare_train_set(df, min_duration=0.3, sr=8000, frame_size=0.6,hop=0.3, n_fft=200, hop_length=80, \n",
    "                      labels={'Neutral':0, 'Frustrated':1, 'Angry':2, 'Happy':3, 'Sad':4}):\n",
    "    x = []\n",
    "    y = []\n",
    "    durations = list(df['duration'])\n",
    "    roots = list(df['root'])\n",
    "    emotions = list(df['emotion'])\n",
    "    with tqdm(total=len(df)) as pbar:\n",
    "        for i in range(len(emotions)):\n",
    "            tmp_mel = read_feature(roots[i], random.random()*(durations[i]-min_duration), sr=sr, frame_size=frame_size, hop=hop, n_fft=n_fft, hop_length=hop_length)\n",
    "            x.append(tmp_mel)\n",
    "            y.append(labels[emotions[i]])\n",
    "            pbar.update(1)\n",
    "    x = np.reshape(np.stack(x), (len(x),1,x[0].shape[0],x[0].shape[1]))\n",
    "    return x, y\n",
    "def eval_result(test_mapper, \n",
    "                emo_pred, \n",
    "                label_filename, \n",
    "                labels=['Neutral', 'Frustrated', 'Angry', 'Happy', 'Sad'],\n",
    "                verbose=0):\n",
    "    df = pd.read_csv(label_filename)\n",
    "    tmp_df = pd.DataFrame({'root':test_mapper['root'],\n",
    "                         'start':test_mapper['start'],\n",
    "                         'end':test_mapper['end'],\n",
    "                         'emotion':test_mapper['emotion'],\n",
    "                         'emo_pred':emo_pred})\n",
    "    y_pred = tmp_df[['root', 'emo_pred']].groupby(['root', 'emo_pred']).size().groupby(level=0).idxmax().apply(lambda x: x[1]).reset_index(name='pred_emotion').rename(columns={'root':'root2'})\n",
    "    df = df.merge(y_pred, left_on='root', right_on='root2').drop('root2',axis=1)\n",
    "    if verbose:\n",
    "        print()\n",
    "        print('################## Error analysis #################################')\n",
    "        print()\n",
    "    conf_mat = confusion_matrix(df['emotion'], df['pred_emotion'], labels=labels)\n",
    "    f1s = f1_score(df['emotion'], df['pred_emotion'],average=None)\n",
    "    if verbose:\n",
    "        print(labels, f1s, 'avg:', np.mean(f1s))\n",
    "        sns.heatmap(conf_mat, xticklabels=labels, yticklabels=labels, annot=True)\n",
    "        plt.show()\n",
    "    # conf_mat = confusion_matrix(df['emotion'], df['pred_emotion'], labels=labels, normalize='true')\n",
    "    # print(conf_mat)\n",
    "    # sns.heatmap(conf_mat, xticklabels=labels, yticklabels=labels, annot=True)\n",
    "    # plt.show()\n",
    "    if verbose:\n",
    "        print()\n",
    "        print('################## Duration analysis #################################')\n",
    "        print()\n",
    "    thres1, thres2 = np.percentile(df['duration'],[33,66])\n",
    "    tmp = df[df['duration']<=thres1][['emotion', 'pred_emotion']]\n",
    "    conf_mat = confusion_matrix(tmp['emotion'], tmp['pred_emotion'], labels=labels)\n",
    "    if verbose:\n",
    "        print('for duration <=', thres1)\n",
    "        sns.heatmap(conf_mat, xticklabels=labels, yticklabels=labels, annot=True)\n",
    "        plt.show()\n",
    "    tmp = df[(df['duration']<=thres2)&(df['duration']>thres1)][['emotion', 'pred_emotion']]\n",
    "\n",
    "    conf_mat = confusion_matrix(tmp['emotion'], tmp['pred_emotion'], labels=labels)\n",
    "    if verbose:\n",
    "        print('for', thres1, '< duration <=', thres2)\n",
    "        sns.heatmap(conf_mat, xticklabels=labels, yticklabels=labels, annot=True)\n",
    "        plt.show()\n",
    "    tmp = df[df['duration']>thres2][['emotion', 'pred_emotion']]\n",
    "    conf_mat = confusion_matrix(tmp['emotion'], tmp['pred_emotion'], labels=labels)\n",
    "    if verbose:\n",
    "        print('for duration >', thres2)\n",
    "        sns.heatmap(conf_mat, xticklabels=labels, yticklabels=labels, annot=True)\n",
    "        plt.show()\n",
    "        print()\n",
    "        print('################## Microphone analysis #################################')\n",
    "        print()\n",
    "    for i in df['microphone'].unique():\n",
    "\n",
    "        tmp = df[df['microphone']==i][['emotion', 'pred_emotion']]\n",
    "        conf_mat = confusion_matrix(tmp['emotion'], tmp['pred_emotion'], labels=labels)\n",
    "        if verbose:\n",
    "              print('for microphone:', i)\n",
    "              sns.heatmap(conf_mat, xticklabels=labels, yticklabels=labels, annot=True)\n",
    "              plt.show()\n",
    "    if verbose:\n",
    "        print()\n",
    "        print('################## Condition analysis #################################')\n",
    "        print()\n",
    "    for i in df['condition'].unique():\n",
    "        tmp = df[df['condition']==i][['emotion', 'pred_emotion']]\n",
    "        conf_mat = confusion_matrix(tmp['emotion'], tmp['pred_emotion'], labels=labels)\n",
    "        if verbose:\n",
    "              print('for condition:', i)\n",
    "              sns.heatmap(conf_mat, xticklabels=labels, yticklabels=labels, annot=True)\n",
    "              plt.show()\n",
    "    return df, f1s\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_shape, num_classes=5):\n",
    "    inputs = keras.layers.Input(shape=input_shape, name=\"input\")\n",
    "\n",
    "    x = keras.layers.Conv2D(64,kernel_size=3,padding=\"same\",activation='relu',use_bias=True,kernel_regularizer=None,bias_regularizer=None,activity_regularizer=None)(inputs)\n",
    "    x = keras.layers.Conv2D(32,kernel_size=3,padding=\"same\",activation='relu',use_bias=True,kernel_regularizer=None,bias_regularizer=None,activity_regularizer=None)(x)\n",
    "    x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "    x = keras.layers.Dense(32, activation=\"relu\")(x)\n",
    "    # x = keras.layers.Dense(128, activation=\"relu\")(x)\n",
    "    outputs = keras.layers.Dense(num_classes, activation=\"softmax\", name=\"output\")(x)\n",
    "    return keras.models.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "model = build_model((1,128, 61), num_classes=5)\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image-processing",
   "language": "python",
   "name": "image_processing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
